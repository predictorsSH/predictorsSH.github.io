I"e<p>크로스엔트로피에 대해서, 알지 못했던 관점들을 설명해주는 글을 읽었습니다.
정말 감명깊게 읽었습니다! 
해당 포스트는 그 글을, 정리 및 공부할 목적으로 작성하였습니다.</p>

<p>해당 글은 아래 블로그에서 보실 수 있습니다.
<a href="https://theeluwin.postype.com/post/6080524">제이미님의 블로그</a></p>

<h2 id="크로스-엔트로피">크로스 엔트로피</h2>

<p>크로스 엔트로피는 분류 모델에서 거의 항상 사용되는 비용함수이다.<br />
어떻게 크로스 엔트로피를 분류 모델의 비용함수로 사용할 수 있을까?<br /></p>

<p>그것은 크로스 엔트로피가</p>
<ul>
  <li>두 분포 사이의 차이를 측정하기 때문이다.</li>
</ul>

<p>머신러닝은 정답 분포를 잘 모사하는 모델을 만드는 것을 목표로한다.<br /> 
다시말해, 우리가 모델링한 모델이 출력하는 분포와 실제 값의 분포 차이가 없어야 좋은 모델이라고 할 수 있다. <br /> 
그런 점에서, 두 분포의 차이를 측정하는 크로스 엔트로피는 좋은 비용함수가 될 수 있는 것이다.<br /></p>

<p>조금더 자세히 설명하면, <br /></p>

<p>$$ CE(p,q) = -\sum_i P(i)logQ(i) $$</p>

<p>크로스 엔트로피 식은 위와 같다. 여기서 $p$는 진분포(정답분포), $q$는 모델의 모사한 분포이다. <br /></p>

<p>우리는 마지막 분류층에서 softmax, sigmoid 활성화 함수를 활용한다. <br />
이 두가지 활성화 함수를 통과한 벡터는 합이 1인 확률 분포와 같아지게 되는데<br />
이때 그 분포가 모델이 모사한 분포 $q$, [1,0,0] 과 같은 정답 레이블이 진분포 $p$가 된다.<br /></p>

<h2 id="kullback-leibler-divergency-쿨백-라이블러-발산">Kullback-Leibler divergency (쿨백-라이블러 발산)</h2>

<p>크로스 엔트로피를 이해하기 앞서, KL-deviergency(KLD)를 먼저 살펴보자.<br />
KLD는 두 확률 분포의 차이를 계산하는데에 사용하는 함수다. <br /></p>

<p>$$D_KL(P \parallel Q) = \sum_i P(i)log (P(i) \over Q(i)) $$</p>
:ET