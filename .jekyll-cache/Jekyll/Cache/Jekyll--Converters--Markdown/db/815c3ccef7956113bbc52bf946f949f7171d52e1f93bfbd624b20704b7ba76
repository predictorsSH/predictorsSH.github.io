I"U
<h2 id="transformer-attention-is-all-you-need">Transformer: Attention Is All You Need</h2>

<p><a href="https://arxiv.org/abs/1706.03762">paper</a><br /></p>

<p>해당 논문을 글로만 읽었을때 남아있는 찝찝함, 가려운 부분을 해소하기 위해서 구현된 코드를 분석하며 공부하였습니다.<br />
딥러닝을 이용한 자연어 처리 입문이라는 교재에서 keras로 구현된 <a href="https://wikidocs.net/31379">코드</a>를 활용하였습니다.<br /></p>

<p><img src="/assets/images/contents/paper/transforemr/full_architecture.PNG" alt="아키텍처" /></p>

<h2 id="positional-encoding">Positional Encoding</h2>

<h4 id="요약">요약</h4>

<p>d_model 차원으로 embedding된 시퀀스 벡터에, Positional Encoding 벡터를 합해준다. <br />
단일 시퀀스 데이터에 대한 Postional Encoding의 출력은 (시퀀스 길이 x d_model(임베딩차원)) 크기의 벡터가 된다.<br /></p>

<p><img src="/assets/images/contents/paper/transforemr/positional_encoding_architecture.PNG" alt="아키텍처" /></p>

<h4 id="설명">설명</h4>
<p>먼저, Positional Encoding은 데이터의 순서정보를 학습하기 위해 사용하는 기법입니다.<br />
transformer가 순환신경망을 사용하지 않고도 시퀀스 데이터를 학습할 수 있는것이 Positional Encoding 덕분입니다.<br /></p>

<p>positional encoding에 대해서 정리하기에 앞서서,해당 내용을 100% 이해하고 작성한 것이 아님을 알립니다.<br /></p>

<p>아키텍처 그림을 보면, input된 데이터를 embedding해주고, 거기에 positional encoding을 단순 합한다고 되어있습니다.<br /></p>

<p>이때, Input되는 데이터는 정수 인코딩된 데이터 입니다. 그러면 Input 데이터의 shape은 (시퀀스 길이, )가 됩니다.<br />
( 실제 (batchsize,시퀀스 길이,)크기의 데이터가 INPUT 됨. 여기서는 배치 사이즈는 생략한 단건의 데이터로 설명)</p>

<p>이제 정수 인코딩 된 데이터를 d 차원으로 embedding 하면 embedding 된 데이터의 크기는 어떻게 될까요?<br />
(시퀀스 길이 , d(임베딩 차원))이 됩니다. <br /></p>

<p>그럼 여기에 Positional Encoding의 출력을 합하기 위해서는, Positional Encoding의 출력 역시 (시퀀스 길이, d)가 되어야 할것입니다.<br />
아래 <a href="https://wikidocs.net/31379">그림</a>은 딥러닝을 이용한 자연어 처리 입문 교재에서 임베딩 벡터와 포지션 인코딩</p>

<p><img src="/assets/images/contents/paper/transforemr/positional_encoding_ex.PNG" alt="아키텍처" /></p>
:ET