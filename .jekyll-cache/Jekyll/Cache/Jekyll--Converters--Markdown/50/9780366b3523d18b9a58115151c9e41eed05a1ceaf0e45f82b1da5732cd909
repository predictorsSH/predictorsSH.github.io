I"<h2 id="transformer-attention-is-all-you-need">Transformer: Attention Is All You Need</h2>

<p><a href="https://arxiv.org/abs/1706.03762">paper</a><br /></p>

<p>해당 논문을 글로만 읽었을때 남아있는 찝찝함, 가려운 부분을 해소하기 위해서 구현된 코드를 분석하며 공부하였습니다.<br />
딥러닝을 이용한 자연어 처리 입문에서 keras로 구현된 <a href="https://wikidocs.net/31379">코드</a>를 활용하였습니다.<br /></p>

<h2 id="positional-encoding">Positional Encoding</h2>

<p>먼저, Positional Encoding은 데이터의 순서정보를 학습하기 위해 사용하는 기법입니다.<br />
transformer가 순환신경망을 사용하지 않고도 시퀀스 데이터를 학습할 수 있는것이 Positional Encoding 덕분입니다.<br /></p>

<p>positional encoding에 대해서 정리하기에 앞서서,해당 내용을 100% 이해하고 작성한 것이 아님을 알립니다.</p>

:ET