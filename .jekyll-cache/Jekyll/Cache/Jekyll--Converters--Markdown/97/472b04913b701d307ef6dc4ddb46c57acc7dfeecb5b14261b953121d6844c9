I"<h2 id="transformer-attention-is-all-you-need">Transformer: Attention Is All You Need</h2>

<p><a href="https://arxiv.org/abs/1706.03762">paper</a><br /></p>

<p>해당 논문을 글로만 읽었을때 남아있는 찝찝함, 가려운 부분을 해소하기 위해서 구현된 코드를 분석하며 공부하였습니다.<br />
딥러닝을 이용한 자연어 처리 입문이라는 교재에서 keras로 구현된 <a href="https://wikidocs.net/31379">참고자료</a>를 활용하였습니다.<br /></p>

<p><img src="/assets/images/contents/paper/transforemr/full_architecture.PNG" alt="아키텍처" /></p>

<h2 id="positional-encoding">Positional Encoding</h2>

<p>positional encoding에 대해서 정리하기에 앞서서,해당 내용을 100% 이해하고 작성한 것이 아님을 알립니다.<br /></p>

<h4 id="요약">요약</h4>
<ul>
  <li>positional Encoding은 시퀀스 데이터에서 각 위치에대한 representation입니다.</li>
  <li>d_model 차원으로 embedding된 시퀀스 데이터 벡터에, Positional Encoding 벡터를 합해준다.(위치 정보를 합해준다.) <br /></li>
  <li>단일 시퀀스 데이터에 대한 Postional Encoding의 출력은 (시퀀스 길이 x d_model(임베딩차원)) 크기의 벡터가 된다.<br /></li>
</ul>

<h4 id="설명">설명</h4>
<p>Positional Encoding은 데이터의 순서정보를 학습하기 위해 사용하는 기법입니다.<br />
transformer가 순환신경망을 사용하지 않고도 시퀀스 데이터를 학습할 수 있는것이 Positional Encoding 덕분입니다.<br /></p>

<p><img src="/assets/images/contents/paper/transforemr/positional_encoding_architecture.PNG" alt="아키텍처" /></p>

<p>아키텍처 그림을 보면, inputs 데이터를 embedding해주고, 거기에 positional encoding을 단순 합한다고 되어있습니다.<br /></p>

<p>이때, Inputs은 정수 인코딩된 데이터 입니다. 그러면 Inputs 데이터의 shape은 (batch_size,시퀀스 길이, )가 됩니다.<br />
단건의 데이터라고 생각하면 shape은 (시퀀스길이, )가 됩니다.</p>

<p>정수 인코딩 된 단건의 데이터를 d 차원으로 embedding 하면 embedding 된 데이터의 크기는 어떻게 될까요?<br />
(시퀀스 길이 , d(임베딩 차원))이 됩니다. <br /></p>

<p>그럼 여기에 Positional Encoding의 출력을 합하기 위해서는, Positional Encoding 역시 (시퀀스 길이, d)가 되어야 할것입니다.<br />
아래 <a href="https://wikidocs.net/31379">그림</a>(출처)은 위 설명을 알기 쉽게 보여줍니다.</p>

<p><img src="/assets/images/contents/paper/transforemr/positional_encoding_ex.PNG" alt="아키텍처" /></p>

<p>그럼, Positional Encoding을 어떻게 수행할까요?<br />
아래 논문에서 소개한 식처럼, sine cosine 함수를 활용합니다.<br /></p>

<p><img src="/assets/images/contents/paper/transforemr/positional_encoding_f.PNG" alt="아키텍처" /></p>

<p>어떻게 위 식이 유도되었을까요?<br />
Positional Encoding은 결국, 시퀀스 데이터에서 특정 위치(인덱스)에 대한 representation입니다. <br />
간단한 예로 시퀀스 길이가 100이라면, 벡터 [0,1,2…99]가 위치(인덱스)에 대한 representation이 될 수도 있겠죠.
하지만 위와 같이 representation을 하면, 학습할 때 문제가 생깁니다! <br />
시퀀스의 길이는 데이터마다 다릅니다. 따라서 길이가 길어지면 인덱스도 같이 커지기 때문에 훈련시 기울기 폭주가 발생할 수도 있습니다.<br />
또한 학습데이터에 비해 테스트 데이터가 길다면, 모델이 길이에 크게 영향을 받게되어 모델의 일반화가 어려워집니다.<br /></p>

<p>그러나 sine cosine 함수를 이용하면 위와 같은 문제를 해결할 수 있게 됩니다.<br /> 
찾아보니 이상적인 positional encoding을 하기 위해서 지켜져야할 조건 등이 있었습니다.<br /></p>

<ol>
  <li>각 위치마다 고유하고 일관된 벡터가 나와야함</li>
  <li>서로 다른 길이의 시퀀스 데이터에 대해서도 적용가능해여함</li>
  <li>서로 다른 길이의 시퀀스의 두 인덱스 간의 거리는 일정해야함(첫번쨰와 두번째 인덱스 차이는 두번째 네번째 차이와 동일해야함)</li>
  <li>임의의 두행에 대해서 선형변환이 가능해야함(Attention 적용을 위함) - &gt; sin cosine을 번갈아 쓰는 이유</li>
</ol>

<p>위 조건들을 만족하는 Positional Encoding이 트랜스포머의 Postional Encoding 입니다.
<a href="https://hongl.tistory.com/231">블로그</a>에서 위 내용을 조금더 자세하게 알아볼수 있습니다.</p>

<h2 id="코드-설명">코드 설명</h2>
<p>Positional Encoding을 구현하는 것은 그렇게 어렵지 않습니다.<br /></p>

:ET