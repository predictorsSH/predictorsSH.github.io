I"<h2 id="transformer-attention-is-all-you-need">Transformer: Attention Is All You Need</h2>

<h3 id="paper"><a href="https://arxiv.org/abs/1706.03762">[paper]</a></h3>

<h2 id="참고자료">참고자료</h2>

<p><a href="https://youtu.be/AA621UofTUA">https://youtu.be/AA621UofTUA</a> <br /> 
동빈나님의 유튜브강의를 듣고 정리한 내용입니다. <br /> 
추가적으로 아래 자료들을 참고하여 공부하였습니다. <br /></p>

<p><a href="https://youtu.be/AA621UofTUA">https://youtu.be/AA621UofTUA</a><br />
<a href="https://wikidocs.net/31379">https://wikidocs.net/31379</a></p>

<h2 id="transformer">Transformer</h2>

<p>트랜스포머는 <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> 논문에서 소개된 모델이다.<br /> 
논문에서는 sequnece to sequence 문제를 풀기 위해 순환신경망을 사용하지 않는 모델을 만들고자 했고, 그 결과물이 <br />
Attention 메커니즘만을 활용한 Transformer 모델이다.</p>

<p>트랜스포머 모델은 Encoder, Decoder, Multi-head Attention, Positional Encoding를 포함하는 구조로 이루어져있다.<br /></p>

<p><img src="/assets/images/contents/paper/transforemr/architecture.PNG" alt="아키텍처" /></p>

<h2 id="positional-encoding">Positional Encoding</h2>

<p>NLP에서 Input된 데이터가 제일 처음 만나는 레이어는 임베딩 레이어이다.임베딩은 투입된 문장을 학습하기에 알맞은 벡터로 변환해 주는 작업이다.<br />
<br />
대부분 문장을 단어단위로 쪼개어, 단어별로 임베딩하게 된다. 이떄, 단어의 위치 정보는 학습에 중요한 역할을 하게 되는데, Transformer은 RNN을 사용하지 않기 때문에 위치 정보를 학습시킬 다른 방법이 필요하다.<br /> 
<br />
그 방법이 바로 Positional Encoding이다.<br />
positional Encoding은 sin, cos 주기함수를 활용해 각 단어의 상대적인 위치를 네트워크에 입력한다.<br /></p>

<p>$$ PE_{(pos,2i)} = sin(pos/10000^{2i/d_model}) $$ $$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_model}) $$</p>

<p>Transformer는 위 사인함수와 코사인 함수의 값을 임베딩 벡터에 더해주므로서 단어의 위치에 대한 정보를 주입한다. 어떻게 위 식이 유도되었는지, 어떻게 위치 정보를 주입할수 있는지를 알고싶다면 이 <a href="https://hongl.tistory.com/231">블로그</a>를 참고하자</p>

<p><img src="/assets/images/contents/paper/transforemr/Attention1.PNG" alt="아키텍처" /></p>

<p>임베딩이 끝나면 Attention을 시작으로 Encoding을 진행한다.</p>

<h2 id="multi-head-attention">Multi-head Attention</h2>
<p>Transformer의 인코더와 디코더는 Multi-head Attention 레이어를 사용한다.<br />
Multi-head Attention은 쉽게 말해서, 여러개의 Attention을 활용하는것이다.<br />
각 Attention이 문장을 서로 다른 관점에서 보고, 그 정보를 잘 통합하여 학습한다면 더 좋은 성능을 낼 수 있을 것이다.<br /></p>

:ET