I"<p>*&lt;핸즈온 머신러닝 2판&gt;을 참고하였습니다.<br />
*<a href="https://wikidocs.net/106473">위키독스</a>를 참고 하였습니다.
*<a href="https://excelsior-cjh.tistory.com/183">참고블로그</a>를 참고하였습니다.</p>

<h2 id="순환-신경망">순환 신경망</h2>

<p>순환신경망은 시퀀스 데이터를 다루는데 특화되어 있다. 대표적인 시퀀스 데이터는 언어이다. <br /> 
한장의 이미지에는 순서가 없다. 모든 픽셀값은 같은 시간에서의 데이터이다.  <br />
그러나 하나의 문장에는 순서가 있다. 문장은 문장의 앞에서부터 뒤로, 순서대로 들어야 제대로 이해 할 수 있다.<br /><br />
이렇게 입력 순서가 예측이나, 분류에 중요한 영향을 주는 경우, DNN이나 CNN으로는 한계가 있을 수 있다. DNN과 CNN은 입력 순서를 고려하지 않기 때문이다.<br />
<br />
순환신경망(RNN)은 이전에 입력된 데이터를 <strong>‘기억’</strong>함으로써 입력 순서를 고려하게 된다. 이전에 입력된 데이터에 새로 입력되는 데이터들을 더하여 새로운 <strong>요약 정보</strong>를 생성한다.<br />
이렇게 순환신경망의 순환층을 통과하여 나온 <strong>요약된 정보</strong>를 이용해 예측 또는 분류와 같은 태스크들을 수행하게 된다.<br /></p>

<h2 id="순환-층">순환 층</h2>
<p>순환 뉴런은 이전에 입력된 데이터를 기억하고, 기억된 정보와 새로운 정보를 합하기위해 당음과 같은 구조를 가진다.<br /></p>

<p><img src="/assets/images/contents/NN/RNN/recurrent_unit.PNG" alt="순환뉴런" /></p>

<ul>
  <li>t-1 시점의 입력 $X_{(t-1)}$가 RNN 셀을 통과하면 $h_{(t-1)}$ (hidden state)와 output 두가지를 각각 만들어낸다.<br /></li>
  <li>그리고 t-1 시점의 hidden state는 다음 시점 t 에서 새로운 Input $X_t$ 와 함께 셀에 투입된다.<br /></li>
  <li>그렇게되면 마찬가지로 $h_t$ (hidden state)와 output 두가지를 각각 만들어내고 이 과정을 하나의 시퀀스가 끝날때까지 반복하게 된다.<br /></li>
</ul>

<p>여기서 $h_t$ 가, 이전 시점의 input에 대한 정보를 계속 저장(기억)하고 있음을 알 수있다.<br />
아래 그림은 위 설명한 과정을 시퀀스대로 펼친것을 표현한 그림이다.<br /></p>

<p>*아래 그림은 <a href="https://excelsior-cjh.tistory.com/183">참고블로그</a> 님의 블로그에서 다운받았습니다.<br /></p>

<p><img src="/assets/images/contents/NN/RNN/rnn_unrolled.PNG" alt="unrolled recurrent neurl network" /></p>

<ul>
  <li>위 그림에서 왼쪽 그림을 타임 스텝에 따라 펼친 그림이 오른쪽 그림이다.</li>
  <li><strong>순환신경망에서 순환 뉴런은 새로운 input 데이터 $x$와 함께 이전 타임스텝의 출력인 $y_{(t-1)}$ 을 받는다. (핸즈온 머신러닝에서는 $h_{(t-1)}$ 대신 $y_{(t-1)}$을 받는다고 하였지만, 일반적인 RNN은 $h_{(t-1)}$을 받는것이 맞음)</strong></li>
  <li>각 순환뉴런은 두벌의 가중치를 가진다. 하나는 $x_{(t)}$를 위한 $w_x$이고 다른 하나는 $y_{(t-1)}$을 위한 $w_y$이다. (역시 $y_{(t-1)}$ 대신 $h_{(t-1)}$을 위한 $w_h$라고 표기하는 것이 일반적)</li>
  <li>
    <p>하나의 샘플에 대한 순환층의 출력은 아래 식과 같이 계산된다. * $\phi$는 활성화 함수</p>

    <p>$$y_{(t)}= \phi(W_x^Tx_{(t)}+W_y^Ty_{(t-1)}+b)$$ 또는<br />
$$h_{(t)}= \phi(W_x^Tx_{(t)}+W_h^Th_{(t-1)}+b)$$<br />
$$Y_t = W^T_y \cdot h_t$$</p>

    <p>라고 표기 할 수 있다.</p>
  </li>
  <li>
    <p>타임 스텝 t에서의 모든 입력을 행렬 $X_{(t)}$로 만들어 미니배치 전체에 대해 순환층의 출력을 한번에 계산할 수 있다.</p>

    <p>$$Y_{(t)} = \phi(X_{(t)}W_x+Y_{(t-1)}Wy+b)$$</p>
  </li>
</ul>

<h2 id="메모리-셀">메모리 셀</h2>

<ul>
  <li>타임 스텝에 걸쳐서 어떤 상태를 보존하는 신경망의 구성 요소를 메모리 셀이라고 한다.</li>
  <li>일반적으로 타임 스텝 t에서의 셀의 상태 $h_{(t)}$는 그 타임 스텝의 입력과 이전 타임 스텝의 상태에 대한 함수이다. 
$h{(t)} = f(h_{(t-1)}, x{(t)})$</li>
  <li>타임 스텝 t에서의 출력 $y_{(t)}$도 이전 상태와 현재 입력에 대한 함수이다.</li>
</ul>

<h2 id="lstm">LSTM</h2>
<p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">내용 참고 및 그림 출처</a></p>

<p>LSTM은 RNN의 the problem of Long-Term Dependencies 문제를 해결하기 위해 고안 되었다.<br />
LSTM의 구조는 아래와 같다. RNN의 구조와 비교해보자<br /></p>

<p><img src="/assets/images/contents/NN/RNN/lstn_rnn.PNG" alt="Lstm structure" /></p>

<p>한눈에 봐도 RNN보다 더 복잡해진 구조를 가지고 있는것을 알 수 있다.<br />
가장 큰 차이를 간단하게 설명하면, LSTM에 Cell State, 그리고 gate들이 추가되었다는 점이다.<br /></p>

<h2 id="lstm-cell-state">LSTM Cell state</h2>
<p>차이를 조금더 자세히 살펴보자.<br />
LSTM의 핵심은 cell state이다.<br /></p>

<h3 id="삭제-게이트">삭제 게이트</h3>

<p><img src="/assets/images/contents/NN/RNN/delete_cell.PNG" alt="Lstm structure" /></p>
:ET