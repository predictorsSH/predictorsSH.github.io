I"<p>크로스엔트로피에 대해서, 알지 못했던 관점들을 설명해주는 글을 읽었습니다.
정말 감명깊게 읽었습니다! 
해당 포스트는 그 글을, 정리 및 공부할 목적으로 작성하였습니다.</p>

<p>해당 글은 아래 블로그에서 보실 수 있습니다.
<a href="https://theeluwin.postype.com/post/6080524">제이미님의 블로그</a></p>

<h2 id="크로스-엔트로피">크로스 엔트로피</h2>

<p>크로스 엔트로피는 분류 모델에서 거의 항상 사용되는 비용함수이다.<br />
어떻게 크로스 엔트로피를 분류 모델의 비용함수로 사용할 수 있을까?<br /></p>

<p>그것은 크로스 엔트로피가</p>
<ul>
  <li>두 분포 사이의 차이를 측정하기 때문이다.</li>
</ul>

<p>머신러닝은 정답 분포를 잘 모사하는 모델을 만드는 것을 목표로한다.<br /> 
다시말해, 우리가 모델링한 모델이 출력하는 분포와 실제 값의 분포 차이가 없어야 좋은 모델이라고 할 수 있다. <br /> 
그런 점에서, 두 분포의 차이를 측정하는 크로스 엔트로피는 좋은 비용함수가 될 수 있는 것이다.<br /></p>

<p>조금더 자세히 설명하면, <br /></p>

<p>$$ CE(p,q) = -\sum_i P(i)logQ(i) $$</p>

<p>크로스 엔트로피 식은 위와 같다. 여기서 $p$는 진분포(정답분포), $q$는 모델의 모사한 분포이다. <br /></p>

<p>우리는 마지막 분류층에서 softmax, sigmoid 활성화 함수를 활용한다. <br />
이 두가지 활성화 함수를 통과한 벡터는 합이 1인 확률 분포와 같아지게 되는데<br />
이때 그 분포가 모델이 모사한 분포 $q$, [1,0,0] 과 같은 정답 레이블이 진분포 $p$가 된다.<br /></p>

<h2 id="kullback-leibler-divergency-쿨백-라이블러-발산">Kullback-Leibler divergency (쿨백-라이블러 발산)</h2>

<p>크로스 엔트로피를 이해하기 앞서, KL-deviergency(KLD)를 먼저 살펴보자.<br />
KLD는 두 확률 분포의 차이를 계산하는데에 사용하는 함수다. <br />
그 식은 다음과 같이 정의된다.<br /></p>

<p>$$D_{KL} (P \parallel Q) = \sum_i P(i)log {P(i) \over Q(i)} $$</p>

<p>이식을 조금 다시 풀어 써보면,<br /></p>

<p>$$  \sum_i P(i)logP(i)  - \sum_i P(i)logQ(i) $$</p>

<p>가 된다. <br /></p>

<p>위 식의 앞쪽 텀은 확률 분포 P의 엔트로피이고, 뒷쪽 텀은 진분포 P와 P를 근사하는 Q분포의 크로스 엔트로피다.<br />
따라서 의미를 해석해보면, 원래의 분포가 가지는 엔트로피와, 원래분포 대신 근사 분포 Q를 사용하는 크로스 엔트로피의 차이가 된다.
만약 Q가 P를 잘 모사한다면, 차이는 0에 수렴할 것이다!</p>

<h2 id="학습시-왜-kld가-아닌-크로스엔트로피를-사용하는-것일까">학습시 왜 KLD가 아닌 크로스엔트로피를 사용하는 것일까</h2>

<p>그러면 학습시, 우리는 왜 KLD가 아닌 크로스 엔트로피를 사용하는가?<br />
결론부터 말하면, KLD 식의 엔트로피는 고정된 값임으로 학습에따라 영향을 받지 않는다.<br />
따라서 크로스 엔트로피를 최소화 하는것이 곧 KLD를 최소화 하는 것과 같아진다.<br /></p>

<p>예를 들어, (제이미님의 예시를 그대로 가져옴)
3가지 클래스 분류 문제에서 진분포는 [1,0,0], [0,1,0], [0,0,1]와 같은 분포를 가진다.<br />
근데 위와 같은 분포에서 엔트로피를 계산하면 0이다 나온다.<br />
$$ Entropy = -\sum_i P(i)logP(i) $$</p>

<p>예를들어 진분포가 [1,0,0] 이라 가정하면,</p>

<p>$$ Entropy = - \sum_{c=1}^C P(i=c)logP(i=c) $$
 $$         = - P(i=c)logP(i=c) $$
 $$         = - 1 * log1 $$
 $$         = 0 $$</p>
:ET