I"L<h2 id="transformer-attention-is-all-you-need">Transformer: Attention Is All You Need</h2>

<h3 id="paper"><a href="https://arxiv.org/abs/1706.03762">[paper]</a></h3>

<h2 id="참고자료">참고자료</h2>

<p><a href="https://youtu.be/AA621UofTUA">https://youtu.be/AA621UofTUA</a> <br /> 
동빈나님의 유튜브강의를 듣고 정리한 내용입니다. <br /> 
추가적으로 아래 자료들을 참고하여 공부하였습니다. <br /></p>

<p><a href="https://youtu.be/AA621UofTUA">https://youtu.be/AA621UofTUA</a><br />
<a href="https://wikidocs.net/31379">https://wikidocs.net/31379</a></p>

<h2 id="transformer">Transformer</h2>

<p>트랜스포머는 <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> 논문에서 소개된 모델이다.<br /> 
논문에서는 sequnece to sequence 문제를 풀기 위해 순환신경망을 사용하지 않는 모델을 만들고자 했고, 그 결과물이 <br />
Attention 메커니즘만을 활용한 Transformer 모델이다.</p>

<p>트랜스포머 모델은 Encoder, Decoder, Multi-head Attention, Positional Encoding를 포함하는 구조로 이루어져있다.<br /></p>

<p><img src="/assets/images/contents/paper/transforemr/architecture.PNG" alt="아키텍처" /></p>

<h2 id="positional-encoding">Positional Encoding</h2>

<p>NLP에서 Input된 데이터가 제일 처음 만나는 레이어는 임베딩 레이어이다.임베딩은 투입된 문장을 학습하기에 알맞은 벡터로 변환해 주는 작업이다.<br />
<br />
대부분 문장을 단어단위로 쪼개어, 단어별로 임베딩하게 된다. 이떄, 단어의 위치 정보는 학습에 중요한 역할을 하게 되는데, Transformer은 RNN을 사용하지 않기 때문에 위치 정보를 학습시킬 다른 방법이 필요하다.<br /> 
<br />
그 방법이 바로 Positional Encoding이다.<br />
positional Encoding은 sin, cos 주기함수를 활용해 각 단어의 상대적인 위치를 네트워크에 입력한다.<br /></p>

<p>$$ PE_{(pos,2i)} = sin(pos/10000^{2i/d_model}) $$ $$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_model}) $$</p>

<p>Transformer는 아래 그림((출처)[])처럼 사인함수와 코사인 함수의 값을 임베딩 벡터에 더해주므로서 단어의 위치에 대한 정보를 주입한다. 어떻게 위 식이 유도되었는지, 어떻게 위치 정보를 주입할수 있는지를 알고싶다면 이 <a href="https://hongl.tistory.com/231">블로그</a>를 참고하자<br />
<br />
<img src="/assets/images/contents/paper/transforemr/positional_encoding.PNG" alt="아키텍처" /> 
<br /></p>

<p>임베딩과 positional encoding된 데이터는 Multi-heat Attention의 input으로 들어간다.
<br />
<img src="/assets/images/contents/paper/transforemr/Attention1.PNG" alt="아키텍처" /></p>

<p>임베딩이 끝나면 Attention을 시작으로 Encoding을 진행한다.</p>

<h2 id="multi-head-attention">Multi-head Attention</h2>
<p>Transformer의 인코더와 디코더는 Multi-head Attention 레이어를 사용한다.<br />
Multi-head Attention은 쉽게 말해서, 여러개의 Attention을 활용하는것이다.<br />
각 Attention이 문장을 서로 다른 관점에서 보고, 그 정보를 잘 통합하여 학습한다면 더 좋은 성능을 낼 수 있을 것이다.<br />
<br />
<img src="/assets/images/contents/paper/transforemr/multi-head-attention.PNG" alt="아키텍처" /></p>

<p>$$ Attention(Q,K,V) = softmax({QK^T \over \sqrt{d_k}})V $$ 
$$ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) $$ 
$$ MultiHead(Q,K,V) = Concat(head_1,…head_h)W^o $$</p>

<p>*h:head의 개수<br />
<br />
<strong>Attention</strong>은 Query와 각각의 Key들을 곱해주고, 그 값을 Key의 d(차원)에 루트를 취해준 값으로 나눠준다. 그리고 Softmax를 취하여, Query가 어떤 Key와 가장 연관이 있는지 확률값을 구해준다. 그 확률값을 Value와 곱해줘서 최종적으로 Attention Value를 만들어낸다.<br />
<br />
Transformer은 Self Attention 기법을 활용하는데, Self Attention은 모든 단어들에 대해서 Q,V,K 벡터를 얻은 후, 각 단어들 마다 Attention 연산을 수행하여
최종적으로 모든 단어들 사이의 연관도를 얻는다.<br /></p>
<bt>

<br />
이때, $\\sqrt{d\_k}$ 로 나누어주는 이유는 softmax 함수에서 gradient vanishing 위험을 줄이기 위해서이다.<br />
또한, Multi-Head Attention은 서로 다른 Attention을 h개 생성하게 되고, 그것을 **$head\_i$** 라 표현한다.  
최종적으로 $head$들을 일자로 쭉 붙여준 뒤, output metrics $W^o$와 곱해서 Multi\_Head Attention의 값을 구하게 된다.
<br />

</bt>
:ET