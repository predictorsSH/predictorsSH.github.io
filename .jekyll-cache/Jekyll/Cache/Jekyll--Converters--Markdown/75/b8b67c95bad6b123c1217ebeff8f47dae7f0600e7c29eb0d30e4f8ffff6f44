I"m<p>크로스엔트로피에 대해서, 알지 못했던 관점들을 설명해주는 글을 읽었습니다.
정말 감명깊게 읽었습니다! 
해당 포스트는 그 글을, 정리 및 공부할 목적으로 작성하였습니다.</p>

<p>해당 글은 아래 블로그에서 보실 수 있습니다.
<a href="https://theeluwin.postype.com/post/6080524">제이미님의 블로그</a></p>

<h2 id="크로스-엔트로피">크로스 엔트로피</h2>

<p>크로스 엔트로피는 분류 모델에서 거의 항상 사용되는 비용함수이다.<br />
어떻게 크로스 엔트로피를 분류 모델의 비용함수로 사용할 수 있을까?<br /></p>

<p>그것은 크로스 엔트로피가</p>
<ul>
  <li>두 분포 사이의 차이를 측정하기 때문이다.</li>
</ul>

<p>머신러닝은 정답 분포를 잘 모사하는 모델을 만드는 것을 목표로한다.<br /> 
다시말해, 우리가 모델링한 모델이 출력하는 분포와 실제 값의 분포 차이가 없어야 좋은 모델이라고 할 수 있다. <br /> 
그런 점에서, 두 분포의 차이를 측정하는 크로스 엔트로피는 좋은 비용함수가 될 수 있는 것이다.<br /></p>

<p>$$ CE(p,q) = -\sum_{i=1}^n P(x_i)logQ(x_i) $$</p>

<p>크로스 엔트로피 식은 위와 같다. 여기서 $p$는 진분포(정답분포), $q$는 모델의 모사한 분포이다. <br /></p>

<p>예를 들어,<br />
우리는 마지막 분류층에서 softmax, sigmoid 활성화 함수를 활용한다. <br />
이 두가지 활성화 함수를 통과한 벡터는 합이 1인 확률 분포와 같아지게 되는데<br />
이때 그 분포가 모델이 모사한 분포 $q$가 되고, [1,0,0] 과 같은 정답 레이블이 진분포 $p$가 된다.<br /></p>

<h2 id="kullback-leibler-divergency-쿨백-라이블러-발산">Kullback-Leibler divergency (쿨백-라이블러 발산)</h2>

<p>크로스 엔트로피를 이해하기 앞서, KL-deviergency(KLD)를 먼저 살펴보자.<br />
KLD는 두 확률 분포의 차이를 계산하는데에 사용하는 함수다. <br />
그 식은 다음과 같이 정의된다.<br /></p>

<p>$$D_{KL} (P \parallel Q) = \sum_{i=1}^n P(x_i)log {P(x_i) \over Q(x_i)} $$</p>

<p>이식을 조금 다시 풀어 써보면,<br /></p>

<p>$$  \sum_{i=1}^n P(x_i)logP(x_i)  - \sum_{i=1}^n P(x_i)logQ(x_i) $$</p>

<p>가 된다. <br /></p>

<p>위 식의 앞쪽 텀은 확률 분포 P의 엔트로피이고, 뒷쪽 텀은 진분포 P와 P를 근사하는 Q분포의 크로스 엔트로피다.<br />
따라서 의미를 해석해보면, 원래의 분포가 가지는 엔트로피와, 원래분포 대신 근사 분포 Q를 사용하는 크로스 엔트로피의 차이가 된다.
만약 Q가 P를 잘 모사한다면, 차이는 0에 수렴할 것이다!</p>

<h2 id="학습시-왜-kld가-아닌-크로스엔트로피를-사용하는-것일까">학습시 왜 KLD가 아닌 크로스엔트로피를 사용하는 것일까</h2>

<p>그러면 학습시, 우리는 왜 KLD가 아닌 크로스 엔트로피를 사용하는가?<br />
결론부터 말하면, KLD 식의 엔트로피는 고정된 값임으로 학습에따라 영향을 받지 않는다.<br />
따라서 크로스 엔트로피를 최소화 하는것이 곧 KLD를 최소화 하는 것과 같아진다.<br /></p>

<p>예를 들어, (제이미님의 예시를 그대로 가져옴) <br />
3가지 클래스(C) 분류 문제에서 진분포는 [1,0,0], [0,1,0], [0,0,1]와 같은 분포를 가질 수 있다.<br />
근데 위와 같은 분포에서 엔트로피를 계산하면 0이다 나온다.<br /></p>

<p>엔트로피 식은 아래와 같은데,<br />
$$ H(X) = -\sum_{i=1}^n P(x_i)logP(x_i) $$</p>

<p>예를들어 진분포가 [1,0,0] 이라 가정하면, 즉 1번 클래스가 정답이라면,<br />
P(X=2)와 P(X=3)은 0임으로, 아래와 같이 식이 정리된다.</p>

<p>\begin{align} 
H(X) &amp; = - \sum_{c=1}^3 P(X=c)logP(X=c)
\newline &amp; = - P(X=1)logP(X=1)
\newline &amp; = - 1 * log1
\newline &amp; = 0 
\end{align}</p>

<p>이렇게 정답이 하나밖에 없는 분류 뮨제에서 엔트로피는 0이 되어버려서, <br />
크로스 엔트로피만 남게된다.</p>

<p>이로써 우리가 왜 크로스 엔트로피를 분류모델에 활용하는지 잘 알 수 있게 되었다!</p>

<h2 id="크로스엔트로피는-분류-모델이-정답을-정확하게-맞출-확률의-하한을-결정한다">크로스엔트로피는 분류 모델이 정답을 정확하게 맞출 확률의 하한을 결정한다.</h2>

<p>사실 제이미님의 글에서 조금더 감명깊었던 부분은 여기에 있다.<br />
위 소제목 그대로, 크로스엔트로피는 분류 모델이 정답을 정확하게 맞출 확률의 하한을 결정한다는 것이다.<br /></p>

<p>결론부터 말하면, 크로스 엔트로피로 아래와 같은 식을 만족시킬 수 있다.<br />
$$ e^{-CE(Y,\hat{Y})} &lt;= P(Y = \hat{Y}) $$</p>

<p>d여기서 $Y$ 는 진분포  $ \hat{Y} $는 모델이 예측한 분포이다.<br />
위식에서 크로스 엔트로피가 0이 되면 두 본포가 같아질 확률이 1이 된다!</p>

<p>어떻게 위와 같은 식이 증멸될까?<br />
볼록함수의 젠슨 부등식을 활용하면 가능하다.</p>

<p>학습시, 실제 라벨은 $Y$ 모델이 예측한 라벨은 $\hat{Y}$로 표현되곤 한다.<br />
여기서도 위와같은 변수명을 그대로 사용하고 $Y$의 분포를 p, $\hat{Y}$의 분포를 q라 하면:<br /></p>

<p>\begin{align}
CE(Y,\hat{Y}) &amp; = - \sum_{c=1}^C P(Y=c)logQ(\hat{Y}=c)
\newline &amp; = \sum_{c=1}^C p(c)(-logq(c))
\newline &amp; =  \sum_{c=1}^C p(c)f(q(c))
\newline &amp; =  E_{c \simeq p}[f(q(c))]
\newline &amp; \ge  f(E_{c \simeq p}[q(c)])   (Jensen)
\newline &amp; =  f(\sum_{c=1}^C p(c)q(c)) <br />
\end{align}</p>

<p>중간에 부등식으로 표현된 부분이 젠센부등식이다.<br />
쉽게 말해 볼록함수에서는 항상 평균점의 함숫값보다 함숫값의 평균이 더 큰데. 그것을 부등식으로 표현한것이 젠센 부등식이다!</p>

<p>어쨌든 제일 맨 아래 f안에 들어있는 식을보자!<br />
그 식을 해석하면 p(c)와 q(c)가 같아질 확률을 의미함을 알 수 있다.<br />
즉, 다음과 같다는 것이다.<br /></p>

<p>$$ f(\sum_{c=1}^C p(c)q(c)) = f(P(Y = \hat{Y})) $$</p>

<p>위식을 아래와 같이 정리하면! <br />
\begin{align}
\newline CE(Y,\hat{Y})&amp; \ge f(P(Y = \hat{Y}))
\newline CE(Y,\hat{Y}) &amp; \ge -logP(Y = \hat{Y})
\newline -CE(Y,\hat{Y}) &amp;\ge logP(Y = \hat{Y})
\newline  e^{-CE(Y,\hat{Y})} &amp; \ge P(Y = \hat{Y}) 
\end{align}</p>

<p>(제이미님의 글을 인용하였습니다.)
위식의 마지막 라인을 보자! 정말 엄청난 식이 나와버린다.<br />
크로스 엔트로피를 나춘다는것이 더이상 추상적인 손실 함수가 아님을 알 수 있다.<br />
이제 신경망을 학습할때 손실함수가 1이 나온다면, 현재 정답률이 1/e 즉 36%라는 것을 추측할 수 도 있다.</p>

:ET