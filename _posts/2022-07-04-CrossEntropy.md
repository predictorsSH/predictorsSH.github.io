---
layout: post
title: 비용함수로써 크로스엔트로피
subtitle: cross entropy
author: san9hyun
categories: AI
banner : /assets/images/banners/post-bg.jpeg
tags: DataScience AI loss function deeplearning classification
---

크로스엔트로피에 대해서, 알지 못했던 관점들을 설명해주는 글을 읽었습니다.
정말 감명깊게 읽었습니다! 
해당 포스트는 그 글을, 정리 및 공부할 목적으로 작성하였습니다.

해당 글은 아래 블로그에서 보실 수 있습니다.
[제이미님의 블로그](https://theeluwin.postype.com/post/6080524) 


## 크로스 엔트로피

크로스 엔트로피는 분류 모델에서 거의 항상 사용되는 비용함수이다.<br>
어떻게 크로스 엔트로피를 분류 모델의 비용함수로 사용할 수 있을까?<br>

그것은 크로스 엔트로피가
- 두 분포 사이의 차이를 측정하기 때문이다.

머신러닝은 정답 분포를 잘 모사하는 모델을 만드는 것을 목표로한다.<br> 
다시말해, 우리가 모델링한 모델이 출력하는 분포와 실제 값의 분포 차이가 없어야 좋은 모델이라고 할 수 있다. <br> 
그런 점에서, 두 분포의 차이를 측정하는 크로스 엔트로피는 좋은 비용함수가 될 수 있는 것이다.<br> 


$$ CE(p,q) = -\sum_{i=1}^n P(x_i)logQ(x_i) $$

크로스 엔트로피 식은 위와 같다. 여기서 $p$는 진분포(정답분포), $q$는 모델의 모사한 분포이다. <br> 

예를 들어,<br>
우리는 마지막 분류층에서 softmax, sigmoid 활성화 함수를 활용한다. <br>
이 두가지 활성화 함수를 통과한 벡터는 합이 1인 확률 분포와 같아지게 되는데<br>
이때 그 분포가 모델이 모사한 분포 $q$가 되고, [1,0,0] 과 같은 정답 레이블이 진분포 $p$가 된다.<br>


## Kullback-Leibler divergency (쿨백-라이블러 발산)

크로스 엔트로피를 이해하기 앞서, KL-deviergency(KLD)를 먼저 살펴보자.<br>
KLD는 두 확률 분포의 차이를 계산하는데에 사용하는 함수다. <br>
그 식은 다음과 같이 정의된다.<br>

$$D_{KL} (P \parallel Q) = \sum_{i=1}^n P(x_i)log {P(x_i) \over Q(x_i)} $$

이식을 조금 다시 풀어 써보면,<br>

$$  \sum_{i=1}^n P(x_i)logP(x_i)  - \sum_{i=1}^n P(x_i)logQ(x_i) $$

가 된다. <br>

위 식의 앞쪽 텀은 확률 분포 P의 엔트로피이고, 뒷쪽 텀은 진분포 P와 P를 근사하는 Q분포의 크로스 엔트로피다.<br>
따라서 의미를 해석해보면, 원래의 분포가 가지는 엔트로피와, 원래분포 대신 근사 분포 Q를 사용하는 크로스 엔트로피의 차이가 된다.
만약 Q가 P를 잘 모사한다면, 차이는 0에 수렴할 것이다!

## 학습시 왜 KLD가 아닌 크로스엔트로피를 사용하는 것일까

그러면 학습시, 우리는 왜 KLD가 아닌 크로스 엔트로피를 사용하는가?<br>
결론부터 말하면, KLD 식의 엔트로피는 고정된 값임으로 학습에따라 영향을 받지 않는다.<br>
따라서 크로스 엔트로피를 최소화 하는것이 곧 KLD를 최소화 하는 것과 같아진다.<br>

예를 들어, (제이미님의 예시를 그대로 가져옴) <br>
3가지 클래스(C) 분류 문제에서 진분포는 [1,0,0], [0,1,0], [0,0,1]와 같은 분포를 가질 수 있다.<br>
근데 위와 같은 분포에서 엔트로피를 계산하면 0이다 나온다.<br>

엔트로피 식은 아래와 같은데,<br>
$$ H(X) = -\sum_{i=1}^n P(x_i)logP(x_i) $$ 

예를들어 진분포가 [1,0,0] 이라 가정하면, 즉 1번 클래스가 정답이라면,<br>
P(X=2)와 P(X=3)은 0임으로, 아래와 같이 식이 정리된다.

\begin{align} 
H(X) & = - \sum_{c=1}^3 P(X=c)logP(X=c)
\newline & = - P(X=1)logP(X=1)
\newline & = - 1 * log1
\newline & = 0 
\end{align}


이렇게 정답이 하나밖에 없는 분류 뮨제에서 엔트로피는 0이 되어버려서, <br>
크로스 엔트로피만 남게된다.

이로써 우리가 왜 크로스 엔트로피를 분류모델에 활용하는지 잘 알 수 있게 되었다!

## 크로스엔트로피는 분류 모델이 정답을 정확하게 맞출 확률의 하한을 결정한다.

사실 제이미님의 글에서 조금더 감명깊었던 부분은 여기에 있다.<br>
위 소제목 그대로, 크로스엔트로피는 분류 모델이 정답을 정확하게 맞출 확률의 하한을 결정한다는 것이다.<br>

결론부터 말하면, 크로스 엔트로피로 아래와 같은 식을 만족시킬 수 있다.<br>
$$ e^{-CE(Y,\hat{Y})} <= P(Y = \hat{Y}) $$

여기서 $Y$ 는 진분포  $ \hat{Y} $는 모델이 예측한 분포이다.<br>
위식에서 크로스 엔트로피가 0이 되면 두 본포가 같아질 확률이 1이 된다!

어떻게 위와 같은 식이 증멸될까?<br>
볼록함수의 젠슨 부등식을 활용하면 가능하다.

학습시, 실제 라벨은 $Y$ 모델이 예측한 라벨은 $\hat{Y}$로 표현되곤 한다.<br>
여기서도 위와같은 변수명을 그대로 사용하고 $Y$의 분포를 p, $\hat{Y}$의 분포를 q라 하면:<br> 

\begin{align}
CE(Y,\hat{Y}) & = - \sum_{c=1}^C P(Y=c)logQ(\hat{Y}=c)
\newline & = \sum_{c=1}^C p(c)(-logq(c))
\newline & =  \sum_{c=1}^C p(c)f(q(c))
\newline & =  E_{c \sim p}[f(q(c))]
\newline & \ge  f(E_{c \sim p}[q(c)])   (Jensen)
\newline & =  f(\sum_{c=1}^C p(c)q(c))   
\end{align}

중간에 부등식으로 표현된 부분이 젠센부등식이다.<br>
쉽게 말해 볼록함수에서는 항상 평균점의 함숫값보다 함숫값의 평균이 더 큰데. 그것을 부등식으로 표현한것이 젠센 부등식이다!

어쨌든 제일 맨 아래 f안에 들어있는 식을보자!<br>
그 식을 해석하면 p(c)와 q(c)가 같아질 확률을 의미함을 알 수 있다.<br>
즉, 다음과 같다는 것이다.<br>

$$ f(\sum_{c=1}^C p(c)q(c)) = f(P(Y = \hat{Y})) $$ 

위 식을 크로스엔트로피와 함꼐 다시 같이 정리하면! <br>
\begin{align}
\newline CE(Y,\hat{Y})& \ge f(P(Y = \hat{Y}))
\newline CE(Y,\hat{Y}) & \ge -logP(Y = \hat{Y})
\newline -CE(Y,\hat{Y}) & \le logP(Y = \hat{Y})
\newline  e^{-CE(Y,\hat{Y})} & \le P(Y = \hat{Y}) 
\end{align}

(제이미님의 글을 인용하였습니다.)
위식의 마지막 라인을 보자! 정말 엄청난 식이 나와버린다.<br>
e를 밑으로 하는 지수함수는 우상향 그래프이다. 따라서 크로스 엔트로피가 커질수록 좌변이 작아진다.<br>

만약, 크로스 엔트로피가 작아져서 0이되면 좌변의 값은 1이된다.<br>
즉. 크로스 엔트로피가 0이면, $Y$와 $\hat{Y}$의 분포가 같을 확률도 최소 1이 된다!

크로스 엔트로피를 나춘다는것이 더이상 추상적인 손실 함수가 아님을 알 수 있다.<br>
이제 신경망을 학습할때 손실함수가 1이 나온다면, 현재 정답률이 1/e 즉 36%라는 것을 추측할 수 도 있다. 

